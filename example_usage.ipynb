{
 "cells": [
  {
   "cell_type": "markdown",
   "source": "## Notes and Tips\\n\\n- **Rate Limiting**: The functions include built-in rate limiting (sleep every 20 requests) to avoid API throttling\\n- **Error Handling**: Failed requests are logged and can be retried separately\\n- **Data Structure**: \\n  - `SYMPHONIES.csv` contains basic symphony metadata\\n  - `OOS.csv` contains detailed symphony information from API\\n  - `BACKTEST.csv` contains full backtest statistics\\n- **File Organization**: Data is organized by date in `bin/SYMPHONIES-YYYY-MM-DD/` directories\"",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Test fetching a single symphony\ntest_symphony_id = \\\"2gX2Ch9EsG5MmJ9yjAvN\\\"  # OG Boring TQQQ Trendz\nprint(f\\\"Testing fetch for symphony: {test_symphony_id}\\\")\\n\\nok, status_code, data = fetch_symphony(test_symphony_id)\\nif ok:\\n    print(f\\\"Success! Symphony name: {data.get('symphony_name', 'Unknown')}\\\")\\n    print(f\\\"Data keys: {list(data.keys())[:5]}...\\\")  # Show first 5 keys\\nelse:\\n    print(f\\\"Failed with status code: {status_code}\\\")\"",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "source": "## Testing Individual Functions\\nTest the utility functions with a small sample:\"",
   "metadata": {}
  },
  {
   "cell_type": "code",
   "source": "# Step 2: Load existing data (if available)\ntry:\n    df_symphonies = pd.read_csv(get_csv_name('SYMPHONIES'))\n    df_oos = pd.read_csv(get_csv_name('OOS'))\n    print(f\"Loaded symphonies: {len(df_symphonies)} rows\")\n    print(f\"Loaded OOS data: {len(df_oos)} rows\")\n    \n    # Display sample data\n    print(\"\\\\nSample symphony data:\")\n    print(df_symphonies[['symphony_sid', 'title', 'name']].head(3))\n    \nexcept FileNotFoundError as e:\n    print(f\"Data files not found: {e}\")\n    print(\"Run the main workflow first to generate data.\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "source": "# Step 1: Set up dates and directories\nstart_date, end_date = get_start_end_date()\nprint(f\"Date range: {start_date} to {end_date}\")\n\n# Check if data directory exists\nimport os\ndata_dir = f\"bin/SYMPHONIES-{end_date}\"\nif os.path.exists(data_dir):\n    print(f\"Data directory exists: {data_dir}\")\nelse:\n    print(f\"Data directory will be created: {data_dir}\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Example Usage of Extracted Utility Functions\n",
    "This notebook shows how the Jupyter notebooks can be updated to use the modular functions."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Imports"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Updated imports for composer_db.ipynb\n",
    "from composer_api import fetch_symphony, fetch_backtest_raw\n",
    "from data_processing import (\n",
    "    get_symphonies, symphonies_to_df, response_to_dataframe,\n",
    "    get_backtest_and_symphony_name, convert_sid_dict_to_df, merge_dicts\n",
    ")\n",
    "from file_utils import get_csv_name, write_json, read_json, get_start_end_date\n",
    "from quant_analysis import calculate_quantstats_metrics, calculate_oos_stats\n",
    "\n",
    "import pandas as pd\n",
    "import time\n",
    "from os import listdir\n",
    "from os.path import isfile, join"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Utility Functions"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Processing Discord exports (replaces cell 3 in composer_db.ipynb)\n",
    "def process_discord_exports(dirpath):\n",
    "    \"\"\"Process all Discord export files and extract symphonies.\"\"\"\n",
    "    jsonfiles = [join(dirpath, f) for f in listdir(dirpath) if isfile(join(dirpath, f))]\n",
    "    \n",
    "    symphonies_dict = {}\n",
    "    for target_file in jsonfiles:\n",
    "        print(f\"Processing file: {target_file}\")\n",
    "        symphonies_dict.update(get_symphonies(target_file))\n",
    "    \n",
    "    return symphonies_dict"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch fetch symphonies (replaces cell 6 in composer_db.ipynb)\n",
    "def batch_fetch_symphonies(symphony_sid_list):\n",
    "    \"\"\"Fetch all symphony data with rate limiting.\"\"\"\n",
    "    response_list = []\n",
    "    failure_list = []\n",
    "    \n",
    "    for idx, sid in enumerate(symphony_sid_list):\n",
    "        if idx % 20 == 0:\n",
    "            print(f'Sleeping at index {idx} with failure {len(failure_list)}')\n",
    "            time.sleep(1)\n",
    "        \n",
    "        ok_status_code, status_code, json_data = fetch_symphony(sid)\n",
    "        if ok_status_code:\n",
    "            response_list.append(json_data)\n",
    "        else:\n",
    "            failure_list.append((idx, sid, status_code))\n",
    "    \n",
    "    return response_list, failure_list"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Batch fetch backtests (replaces cell 14 in composer_db.ipynb)\n",
    "def batch_fetch_backtests(df, start_date, end_date):\n",
    "    \"\"\"Fetch all backtest data with rate limiting.\"\"\"\n",
    "    rlist, flist = [], []\n",
    "    \n",
    "    for idx, row in df.iterrows():\n",
    "        if idx % 20 == 0:\n",
    "            print(f'Sleeping at index {idx} with failure {len(flist)}')\n",
    "            time.sleep(1)\n",
    "        \n",
    "        sid = row['symphony_sid']\n",
    "        filename = f\"bin/BT-{end_date}/{sid}.json\"\n",
    "        ok_status_code, status_code, jsond = fetch_backtest_raw(sid, start_date, end_date)\n",
    "        write_json(jsond, filename)\n",
    "        \n",
    "        if ok_status_code:\n",
    "            rlist.append(jsond)\n",
    "        else:\n",
    "            flist.append((idx, sid, status_code))\n",
    "    \n",
    "    return rlist, flist"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Process backtest results (replaces cell 16 in composer_db.ipynb)\n",
    "def process_backtest_results(jsonfiles):\n",
    "    \"\"\"Process all backtest JSON files.\"\"\"\n",
    "    dict_allocation, dict_return, dict_stats, dict_name = {}, {}, {}, {}\n",
    "    \n",
    "    for index, jsonfile in enumerate(jsonfiles):\n",
    "        if index % 20 == 0:\n",
    "            print(f'Index: {index}')\n",
    "        try:\n",
    "            jsond = read_json(jsonfile)\n",
    "            df_allocations, df_return, stats, symphony_name, id = get_backtest_and_symphony_name(jsond)\n",
    "            dict_name[id] = symphony_name\n",
    "            dict_stats[id] = stats\n",
    "            dict_return[id] = df_return\n",
    "            dict_allocation[id] = df_allocations\n",
    "        except:\n",
    "            print(f'Fail jsonfile {jsonfile}')\n",
    "    \n",
    "    return dict_allocation, dict_return, dict_stats, dict_name"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Main Workflow"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Complete data collection workflow\n",
    "def main_data_collection_workflow():\n",
    "    \"\"\"Complete data collection workflow.\"\"\"\n",
    "    start_date, end_date = get_start_end_date()\n",
    "    \n",
    "    # Process Discord exports\n",
    "    dirpath = '~/source/discord/archive/20241229'\n",
    "    symphonies_dict = process_discord_exports(dirpath)\n",
    "    \n",
    "    # Convert to DataFrame and save\n",
    "    df = symphonies_to_df(symphonies_dict)\n",
    "    df.to_csv(get_csv_name('SYMPHONIES'))\n",
    "    \n",
    "    # Fetch symphony data\n",
    "    symphony_sid_list = df['symphony_sid'].values\n",
    "    response_list, failure_list = batch_fetch_symphonies(symphony_sid_list)\n",
    "    \n",
    "    # Convert responses to DataFrame and save\n",
    "    df_response = response_to_dataframe(response_list)\n",
    "    df_response.to_csv(get_csv_name('OOS'))\n",
    "    \n",
    "    # Fetch backtest data\n",
    "    rlist, flist = batch_fetch_backtests(df, start_date, end_date)\n",
    "    \n",
    "    # Process backtest results\n",
    "    jsonfiles = [f\"bin/BT-{end_date}/{sid}.json\" for sid in df['symphony_sid'].values]\n",
    "    dict_allocation, dict_return, dict_stats, dict_name = process_backtest_results(jsonfiles)\n",
    "    \n",
    "    # Calculate quantstats metrics\n",
    "    dict_quant_stats = calculate_quantstats_metrics(dict_return)\n",
    "    \n",
    "    # Calculate OOS stats\n",
    "    sid_to_oos = dict(df_response[['symphony_sid', 'backtest_start_date']].values)\n",
    "    dict_quant_oos_stats = calculate_oos_stats(dict_return, sid_to_oos)\n",
    "    \n",
    "    # Save final results\n",
    "    dict_backtest_stats = merge_dicts(dict_stats, dict_quant_stats)\n",
    "    df_backtest_stats = convert_sid_dict_to_df(dict_name, dict_backtest_stats)\n",
    "    df_backtest_stats.to_csv(get_csv_name('BACKTEST'))\n",
    "    \n",
    "    df_oos_stats = convert_sid_dict_to_df(dict_name, dict_quant_oos_stats)\n",
    "    df_oos_stats.to_csv(get_csv_name('OOS'))\n",
    "    \n",
    "    return df_backtest_stats, df_oos_stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Example Usage"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Run the complete workflow\n",
    "df_backtest_stats, df_oos_stats = main_data_collection_workflow()\n",
    "\n",
    "# Display basic statistics\n",
    "print(f\"Backtest stats shape: {df_backtest_stats.shape}\")\n",
    "print(f\"OOS stats shape: {df_oos_stats.shape}\")\n",
    "\n",
    "# Display first few rows\n",
    "df_backtest_stats.head()"
   ]
  },
  {
   "cell_type": "markdown",
   "source": "## Alternative: Step-by-Step Execution\\nFor testing and debugging, you can run each step individually:\"",
   "metadata": {}
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.5"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}