{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 1: Export all the conversations from discord\n",
    "## Step 2: Find all the json responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "dirpath = '~/source/discord/archive/20241229'\n",
    "jsonfiles = [join(dirpath, f) for f in listdir(dirpath) if isfile(join(dirpath, f))]"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 3: Get all symphonies URLs from the json responses"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "\n",
    "def get_symphony_id(url):\n",
    "    if url.endswith('/details'):\n",
    "        return url.split('/')[-2]\n",
    "    else :\n",
    "        return url.split('/')[-1]\n",
    "\n",
    "def get_symphonies(filename):\n",
    "    with open(filename, 'r') as file:\n",
    "        data = json.load(file)\n",
    "\n",
    "    messages = data['messages']\n",
    "    symphonies_dict = {}\n",
    "\n",
    "    embeds = []\n",
    "    for message in messages:\n",
    "        name = message['author']['name']\n",
    "        for embed in message['embeds']:\n",
    "            fields = [field for field in embed['fields'] if field['name'] == 'Author']\n",
    "            name = fields[0]['value'] if len(fields) > 0 else name\n",
    "            embeds.append((embed, name))\n",
    "\n",
    "    for embed, name in embeds:\n",
    "        if 'url' not in embed or embed['url'] is None:\n",
    "            continue\n",
    "        if 'app.composer.trade/symphony' not in embed['url']:\n",
    "            continue\n",
    "        sid = get_symphony_id(embed['url'])\n",
    "        symphonies_dict[sid] = {\n",
    "            'title': embed['title'],\n",
    "            'url': embed['url'],\n",
    "            'timestamp': embed['timestamp'],\n",
    "            'id': sid,\n",
    "            'author': name\n",
    "        }\n",
    "\n",
    "    return symphonies_dict\n",
    "\n",
    "def symphonies_to_df(symphonies_dict):\n",
    "    titles, urls, timestamps, ids, names = [],[],[],[],[]\n",
    "    for _, value in symphonies_dict.items():\n",
    "        titles.append(value['title'])\n",
    "        urls.append(value['url'])\n",
    "        timestamps.append(value['timestamp'])\n",
    "        ids.append(value['id'])\n",
    "        names.append(value['author'])\n",
    "    return pd.DataFrame(\n",
    "        data={\n",
    "            \"symphony_sid\": np.array(ids),\n",
    "            \"title\":np.array(titles),\n",
    "            \"url\":np.array(urls), \n",
    "            \"timestamp\":np.array(timestamps),\n",
    "            \"name\": np.array(names)\n",
    "        }\n",
    "    )\n",
    "\n",
    "symphonies_dict = {}\n",
    "for target_file in jsonfiles:\n",
    "    print(\"Processing file: {}\".format(target_file))\n",
    "    symphonies_dict.update(get_symphonies(target_file))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 4: Export URL info into dataframe\n",
    "## Step 5: Fetch all of them into memories"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "df = symphonies_to_df(symphonies_dict)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import requests\n",
    "import time\n",
    "\n",
    "def fetch_symphony(id, retry=3):\n",
    "  if retry == 0:\n",
    "    return False, 404, {}\n",
    "  try:\n",
    "    ok, status_code, data =  _fetch_symphony(id)\n",
    "    if status_code == 429:\n",
    "      print('Retry {}'.format(id))\n",
    "      time.sleep(10)\n",
    "      return fetch_symphony(id, retry-1)\n",
    "    else: \n",
    "      return ok, status_code, data\n",
    "  except:\n",
    "    print('Retry {}'.format(id))\n",
    "    time.sleep(1)\n",
    "    return fetch_symphony(id, retry-1)\n",
    "\n",
    "def _fetch_symphony(id):\n",
    "  if id.endswith('/details'):\n",
    "    id = id.split('/')[-2]\n",
    "  else :\n",
    "    id = id.split('/')[-1]\n",
    "    \n",
    "  url = f\"https://backtest-api.composer.trade/api/v1/public/symphonies/{id}\"\n",
    "\n",
    "  data = requests.get(url)\n",
    "  return data.ok, data.status_code, data.json()\n",
    "\n",
    "def response_to_dataframe(symphony_data_list):\n",
    "  symphony_dict = {}\n",
    "  for symphony_data in symphony_data_list:\n",
    "    for key, value in symphony_data.items():\n",
    "      symphony_dict[key] = symphony_dict.get(key, [])\n",
    "      symphony_dict[key].append(value)\n",
    "  return pd.DataFrame.from_dict(symphony_dict)\n",
    "\n",
    "symphony_sid_list = df['symphony_sid'].values\n",
    "response_list = []\n",
    "failure_list = []\n",
    "for idx, sid in enumerate(symphony_sid_list):\n",
    "  if idx % 20 == 0:\n",
    "    print('Sleeping at index {} with failure {}'.format(idx, len(failure_list)))\n",
    "    time.sleep(1)\n",
    "  ok_status_code, status_code, json_data = fetch_symphony(sid)\n",
    "  if ok_status_code:\n",
    "    response_list.append(json_data)\n",
    "  else:\n",
    "    failure_list.append((idx, sid, status_code))\n",
    "\n",
    "df_response = response_to_dataframe(response_list)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Refetch the failure ones\n",
    "response_sid_set = set([res['symphony_sid'] for res in response_list])\n",
    "failure_sid_list = set([res[1] for res in failure_list])\n",
    "\n",
    "for idx, sid in enumerate(symphony_sid_list):\n",
    "  if sid in response_sid_set or sid in failure_sid_list:\n",
    "    continue\n",
    "  if idx % 20 == 0:\n",
    "    print('Sleeping at index {} with failure {}'.format(idx, len(failure_list)))\n",
    "    time.sleep(1)\n",
    "  ok_status_code, status_code, json_data = fetch_symphony(sid)\n",
    "  if ok_status_code:\n",
    "    response_list.append(json_data)\n",
    "  else:\n",
    "    failure_list.append((idx, sid, status_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 6: Export DF as OOS"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "from datetime import date, datetime, timedelta\n",
    "\n",
    "def write_json(json_data, filename):\n",
    "    os.makedirs(os.path.dirname(filename), exist_ok=True)\n",
    "    with open(filename, 'w', encoding='utf-8') as f:\n",
    "        json.dump(json_data, f, ensure_ascii=False, indent=4)\n",
    "\n",
    "def read_json(filename):\n",
    "    with open(filename) as f:\n",
    "        return json.load(f)\n",
    "    return None\n",
    "\n",
    "def get_csv_name(filename, end_date=None):\n",
    "    if end_date is None:\n",
    "        _, end_date = get_start_end_date()\n",
    "    path = \"bin/SYMPHONIES-{}/{}.csv\".format(end_date, filename)\n",
    "    os.makedirs(os.path.dirname(path), exist_ok=True)\n",
    "    return path\n",
    "\n",
    "def get_start_end_date():\n",
    "  today = date.today().strftime('%Y-%m-%d')\n",
    "  start_date = '2000-01-01'\n",
    "  end_date = today\n",
    "  return start_date, end_date\n",
    "\n",
    "df.to_csv(get_csv_name('SYMPHONIES'))\n",
    "df_response = response_to_dataframe(response_list)\n",
    "df_response.to_csv(get_csv_name('OOS'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 134,
   "metadata": {},
   "outputs": [],
   "source": [
    "oos_name = get_csv_name('OOS', end_date = '2024-12-30')\n",
    "df_response = pd.read_csv(oos_name)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Health check\n",
    "sid = set(df_response['symphony_sid'])\n",
    "copies_sid = set(df_response['copied_from_symphony_sid'].dropna())\n",
    "print(\"All: {} and Children: {}\".format(len(sid), len(copies_sid)))\n",
    "\n",
    "include_in_table = copies_sid.intersection(sid)\n",
    "not_include_in_table = copies_sid - include_in_table\n",
    "print(\"Include: {} and Not Include: {}\".format(len(include_in_table), len(not_include_in_table)))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 7: Run backtest on all the smphonies"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# borrowed from https://github.com/howardr/polyphony/blob/main/src/composer.py\n",
    "import json\n",
    "import os\n",
    "from datetime import date, datetime, timedelta\n",
    "import requests\n",
    "\n",
    "\n",
    "def fetch_backtest_raw(id, start_date, end_date, retry=3):\n",
    "  if retry == 0:\n",
    "    return False, 404, {}\n",
    "  try:\n",
    "    ok, status_code, data =  _fetch_backtest_raw(id, start_date, end_date)\n",
    "    if status_code == 429:\n",
    "      print('Retry {}'.format(id))\n",
    "      time.sleep(10)\n",
    "      return fetch_backtest_raw(id, start_date, end_date, retry-1)\n",
    "    else: \n",
    "      return ok, status_code, data\n",
    "  except:\n",
    "    print('Retry {}'.format(id))\n",
    "    time.sleep(1)\n",
    "    return fetch_backtest_raw(id, start_date, end_date, retry-1)\n",
    "\n",
    "def _fetch_backtest_raw(id, start_date, end_date):\n",
    "  if id.endswith('/details'):\n",
    "    id = id.split('/')[-2]\n",
    "  else :\n",
    "    id = id.split('/')[-1]\n",
    "\n",
    "  payload = {\n",
    "    \"capital\": 100000,\n",
    "    \"apply_reg_fee\": True,\n",
    "    \"apply_taf_fee\": True,\n",
    "    \"backtest_version\": \"v2\",\n",
    "    \"slippage_percent\": 0.0005,\n",
    "    \"start_date\": start_date,\n",
    "    \"end_date\": end_date,\n",
    "  }\n",
    "\n",
    "  url = f\"https://backtest-api.composer.trade/api/v2/public/symphonies/{id}/backtest\"\n",
    "  print(\"{} {} {} {}\".format(id, start_date, end_date, url))\n",
    "\n",
    "  data = requests.post(url, json=payload)\n",
    "  # return data.json()\n",
    "  return data.ok, data.status_code, data.json()\n",
    "\n",
    "def get_backtest_and_symphony_name(jsond):\n",
    "  id = list(jsond['legend'].keys())[0]\n",
    "  symphony_name = jsond['legend'][id]['name']\n",
    "\n",
    "  holdings = jsond[\"last_market_days_holdings\"]\n",
    "  start_date = convert_trading_date(jsond[\"first_day\"])\n",
    "  end_date = convert_trading_date(jsond[\"last_market_day\"])\n",
    "\n",
    "  tickers = []\n",
    "  for ticker in holdings:\n",
    "    tickers.append(ticker)\n",
    "\n",
    "  allocations = jsond[\"tdvm_weights\"]\n",
    "  date_range = pd.date_range(start=start_date, end=end_date)\n",
    "  df_allocations = pd.DataFrame(0.0, index=date_range, columns=tickers)\n",
    "\n",
    "  for ticker in allocations:\n",
    "    for date_int in allocations[ticker]:\n",
    "      trading_date = convert_trading_date(date_int)\n",
    "      percent = allocations[ticker][date_int]\n",
    "      df_allocations.at[trading_date, ticker] = percent\n",
    "\n",
    "  capital_return = jsond['dvm_capital']\n",
    "  df_return = pd.DataFrame(0.0, index=date_range, columns=['capital'])\n",
    "  for date_int in capital_return[id]:\n",
    "    trading_date = convert_trading_date(date_int)\n",
    "    df_return.at[trading_date, 'capital'] = capital_return[id][date_int]\n",
    "\n",
    "  stats = jsond['stats']\n",
    "  \n",
    "  return df_allocations, df_return, stats, symphony_name, id\n",
    "\n",
    "def convert_trading_date(date_int):\n",
    "  date_1 = datetime.strptime(\"01/01/1970\", \"%m/%d/%Y\")\n",
    "  dt = date_1 + timedelta(days=int(date_int))\n",
    "  return dt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "start_date, end_date = get_start_end_date()\n",
    "\n",
    "rlist, flist = [], []\n",
    "for idx, row in df.iterrows():\n",
    "    if idx % 20 == 0:\n",
    "        print('Sleeping at index {} with failure {}'.format(idx, len(flist)))\n",
    "        time.sleep(1)\n",
    "    sid = row['symphony_sid']\n",
    "    filename = \"bin/BT-{}/{}.json\".format(end_date, sid)\n",
    "    # print(\"Backtesting {} and writing to {}\".format(sid, filename))\n",
    "    ok_status_code, status_code, jsond = fetch_backtest_raw(row['symphony_sid'], start_date, end_date)\n",
    "    write_json(jsond, filename)\n",
    "    if ok_status_code:\n",
    "        rlist.append(json_data)\n",
    "    else:\n",
    "        flist.append((idx, sid, status_code))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Step 8: Export the backtest information "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "from os import listdir\n",
    "from os.path import isfile, join\n",
    "\n",
    "dirpath = 'bin/BT-2024-12-30/'\n",
    "jsonfiles = [join(dirpath, f) for f in listdir(dirpath) if isfile(join(dirpath, f))]\n",
    "\n",
    "dict_allocation, dict_return, dict_stats, dict_name = {}, {}, {}, {}\n",
    "for index, jsonfile in enumerate(jsonfiles):\n",
    "    if index % 20 == 0:\n",
    "        print('Index: {}'.format(index))\n",
    "    try:\n",
    "        jsond = read_json(jsonfile)\n",
    "        df_allocations, df_return, stats, symphony_name, id = get_backtest_and_symphony_name(jsond)\n",
    "        dict_name[id] = symphony_name\n",
    "        dict_stats[id] = stats\n",
    "        dict_return[id] = df_return\n",
    "        dict_allocation[id] = df_allocations\n",
    "    except:\n",
    "        print('Fail jsonfile {}'.format(jsonfile))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 140,
   "metadata": {},
   "outputs": [],
   "source": [
    "import quantstats as qs\n",
    "def get_portfolio_values(df):\n",
    "    # get rid of data prior to start of backtest and non-trading days\n",
    "    df = df.loc[(df != 0).any(axis=1)]\n",
    "    return df.iloc[:,0]\n",
    "\n",
    "# portfolio_values = get_portfolio_values(dict_return['jLlgKCXl1P4iP84bSmC1'])\n",
    "# qs.stats.smart_sharpe(portfolio_values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import quantstats as qs\n",
    "import inspect\n",
    "\n",
    "dict_quant_stats = {}\n",
    "qs.extend_pandas()\n",
    "skip_func = {'comp','compsum', 'distribution', 'implied_volatility', 'monthly_returns', \n",
    "             'to_drawdown_series', 'outliers', 'remove_outliers', 'rolling_sharpe', 'rolling_sortino',\n",
    "             'rolling_volatility', 'pct_rank'}\n",
    "function_names = [f for f in dir(qs.stats) if f[0] != '_' and f not in skip_func]\n",
    "\n",
    "for key, value in dict_return.items():\n",
    "    portfolio_values = get_portfolio_values(value)\n",
    "    benchmark = 'SPY'\n",
    "    temp_dict = {}\n",
    "    benchmark_func = []\n",
    "    for function_name in function_names:\n",
    "        func = getattr(qs.stats, function_name)\n",
    "        args = set(inspect.getfullargspec(func).args)\n",
    "        if 'returns' in args or 'prices' in args:\n",
    "            if 'benchmark' not in args:\n",
    "                temp_dict[function_name] = func(portfolio_values)\n",
    "    dict_quant_stats[key] =  temp_dict           \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 142,
   "metadata": {},
   "outputs": [],
   "source": [
    "def extract_date(row):\n",
    "    return pd.to_datetime(row['last_semantic_update_at'][:10])\n",
    "df_response['backtest_start_date'] = df_response.apply(extract_date, axis=1)\n",
    "sid_to_oos = dict(df_response[['symphony_sid', 'backtest_start_date']].values)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_quant_oos_stats = {}\n",
    "\n",
    "for key, value in dict_return.items():\n",
    "    oos_start_date = sid_to_oos[key]\n",
    "    # portfolio_values = value.loc[oos_start_date:].iloc[:,0]\n",
    "    portfolio_values = get_portfolio_values(value.loc[oos_start_date:])\n",
    "    benchmark = 'SPY'\n",
    "    temp_dict = {}\n",
    "    benchmark_func = []\n",
    "    for function_name in function_names:\n",
    "        func = getattr(qs.stats, function_name)\n",
    "        args = set(inspect.getfullargspec(func).args)\n",
    "        if 'returns' in args or 'prices' in args:\n",
    "            if 'benchmark' not in args:\n",
    "                try:\n",
    "                    temp_dict[function_name] = func(portfolio_values)\n",
    "                except:\n",
    "                    temp_dict[function_name] = None\n",
    "    dict_quant_oos_stats[key] =  temp_dict          "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 144,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_allocation, dict_return, dict_stats, dict_name, dict_quant_oos_stats, dict_quant_stats\n",
    "def convert_sid_dict_to_df(sid_to_name, sid_to_values):\n",
    "    columns = set()\n",
    "    df_dict = { 'sid': [], 'name': [] }\n",
    "    for sid, values_in_dict in sid_to_values.items():\n",
    "        columns = columns.union(values_in_dict.keys())\n",
    "    for column in columns:\n",
    "        df_dict[column] = []\n",
    "    for sid, name in sid_to_name.items():\n",
    "        if sid not in sid_to_values:\n",
    "            print('SID {} {} not in backtest.'.format(sid, name))\n",
    "            continue\n",
    "        backtest_stats = sid_to_values[sid]\n",
    "        df_dict['sid'].append(sid)\n",
    "        df_dict['name'].append(name)\n",
    "        for column in columns:\n",
    "            if column in backtest_stats:\n",
    "                df_dict[column].append(backtest_stats[column])\n",
    "            else:\n",
    "                df_dict[column].append(None)\n",
    "    return pd.DataFrame(df_dict)\n",
    "\n",
    "\n",
    "def merge_dicts(dict1, dict2):\n",
    "    result = dict1.copy()\n",
    "    for key, value in dict2.items():\n",
    "        if key in result:\n",
    "            result[key].update(value)\n",
    "        else:\n",
    "            result[key] = value\n",
    "    return result"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 145,
   "metadata": {},
   "outputs": [],
   "source": [
    "dict_backtest_stats = merge_dicts(dict_stats, dict_quant_stats)\n",
    "df_backtest_stats = convert_sid_dict_to_df(dict_name, dict_backtest_stats)\n",
    "df_backtest_stats.to_csv(get_csv_name('BACKTEST'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 146,
   "metadata": {},
   "outputs": [],
   "source": [
    "df_oos_stats = convert_sid_dict_to_df(dict_name, dict_quant_oos_stats)\n",
    "df_oos_stats.to_csv(get_csv_name('OOS'))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# dict_quant_stats['apfULtEPpM7VqWodougq']\n",
    "dict_quant_stats.columns"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.9.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
